---
title: "MATH 3190 Homework 3"
author: "Focus: Notes 6"
date: "Due March 4, 2024"
output: pdf_document
header-includes:
   - \usepackage{multirow}
editor_options: 
  chunk_output_type: console
urlcolor: blue
---

Your homework should be completed in R Markdown or Quarto and Knitted to an html or pdf document. You will ``turn in" this homework by uploading to your GitHub Math_3190_Assignment repository in the Homework directory.


# Problem 1 (19 points) 

### Part a (16 points)
Suppose we are attempting to predict a person's ability to run a marathon in under 4 hours (coded as a 1) based on a number of factors: age, sex, BMI, and blood pressure. Below is the confusion matrix in this situation:

\begin{table}[h]
\centering
\begin{tabular}{ccccc}
                           &                        & \multicolumn{2}{c}{Actual}                         &  \\
                           &                        & 1                       & 0                        &  \\ \cline{3-4}
\multirow{2}{*}{Predicted} & \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{58} & \multicolumn{1}{c|}{102} &  \\ \cline{3-4}
                           & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{37} & \multicolumn{1}{c|}{217} &  \\ \cline{3-4}
\end{tabular}
\end{table}

Find each of the following. Use proper formatting in R Markdown when you type your answers. You can put equations between dollar signs (`$$`) and you can use the `\frac{}{}` (for a small fraction) or `\dfrac{}{}` (for a larger one) commands to nicely type fractions.

* The prevalence of those that can run a mile under 4 hours. 
* The overall accuracy of these predictions.
* The sensitivity (recall).
* The specificity.
* The positive predictive value (precision).
* The negative predictive value.
* The balanced accuracy. 
* Cohen's Kappa ($\kappa$). Check out this link on [Wikipedia](https://en.wikipedia.org/wiki/Cohen%27s_kappa) and scroll down to the section entitled **Binary classification confusion matrix**.


### Part b (3 points)
Read more of the Wikipedia article on Cohen's Kappa, especially the **Interpreting magnitude** and the **Limitations** part. I cannot really verify that you did this, so this is on your honor.



# Problem 2 (81 points)

The `adult` dataset (from the UC Irvine [database](https://archive.ics.uci.edu/dataset/2/adult)) is one that is used to predict whether a person makes over $50K a year based on some other variables. The data came from the Census Bureau in 1994 and can be found in the Data folder in my Math3190_S24 GitHub repo. More info on the dataset can be found in the "adult.names" file.

### Part a (5 points)
Read the data into **R** as a tibble, change the column names to be descriptive about what the variable in that column is, and change the one containing salary information to a factor. Read the "adult.names" file to see the column names.


### Part b (4 points)
Randomly split the dataset into a training and a testing group. Let's use 4/5 of it for training and 1/5 for testing. You can do this with any function you'd like. Please set a seed before you do this so the results are reproducible. 


### Part c (5 points)
Fit two models for predicting whether a person's salary is above $50K or not:

In the first, fit a logistic regression model using the `glm()` function with the `family` set to `"binomial"`. Use `age`, `education`, `race`, `sex`, and `hours_per_week` as the predictors. 

In the second, fit a $k$ nearest neighbors model with $k=7$ neighbors using the `knn3()` function in the `caret` package. Again, use `age`, `education`, `race`, `sex`, and `hours_per_week` as the predictors. 


### Part d (5 points)
With logistic regression, the most common cutoff value for the predicted probability for predicting a "success" is 0.5. Using 0.5 as this cutoff (above 0.5 should be labeled as ">50K"), obtain the class predictions and convert the variable to a factor. You can use the `predict()` function with `type = "response"` to obtain the predicted probabilities of being in the ">50K" group and then compare those probabilities to 0.5. Then use the `confusionMatrix()` function in the `caret` package to obtain the confusion matrix and many associated statistics. Print all of the output from that function.


### Part e (4 points)
Obtain the class predictions for your kNN model and output the results of the `confusionMatrix()` function for this. Note that it will take a few seconds to obtain the predictions for the kNN model.


### Part f (5 points)
Using the output from parts d and e, write a few sentences comparing and contrasting the strengths and weaknesses of each model when it comes to predictions.


### Part g (8 points)
Using the `train()` function in the `caret` package, perform 5-fold cross validation for $k$ in the kNN model using only the training set and again using `age`, `education`, `race`, `sex`, and `hours_per_week` as the predictors. Set the search for $k$ to be from 1 to 21 (we'll stop at 21 to save time). Make sure to use the `trControl` option to set it to cross validation. Then use Cohen's $\kappa$ to determine the best $k$ value. You do not need to change the metric in the `train()` function. Just look at the output and select the $k$ with the largest $\kappa$ value. 

Then, if the best $k$ is different than 7, fit another kNN model with the optimal $k$ value. Please set a seed at the beginning of this code chunk.

It is fairly computationally expensive to optimize the $k$ for the kNN model here since it takes so long to obtain the predictions. So, this may take a few minutes to run. 



### Part h (20 points)
We mentioned the most common cutoff value for the predicted probability for predicting a "success" in logistic regression is 0.5. However, we can adjust this value to make it easier or more difficult to predict a success. Let's optimize this cutoff value using 5-fold cross validation. Note: we could also do this with kNN, but we will not on this assignment.

Using the cutoff values from 0.15 to 0.85 by 0.05 (0.15, 0.20, 0.25, and so on up to 0.85) for predicting whether an adult has a salary above 50K, find which one performs best on the training set using the metric of Cohen's $\kappa$, which is given in the output of the `confusionMatrix()` function. You will need a couple loops here since the `train()` function cannot do this for us. Note: you can find the indices of the rows in each fold using the `createFolds()` function in the `caret` library. Please set a seed at the beginning of your code chunk for this part.


### Part i (5 points)
Once you have your "optimal" cutoff value, repeat part d using this cutoff and compare the results of this output to the results of the output for a kNN model with the optimal $k$ value you found in part g. For which statistics is the logistic regression better now and for which is it worse?


### Part j (15 points)
Finally, let's test our two models (the logistic model with the "optimal" cutoff and the kNN model with the "optimal" $k$) on the test set. We must keep a few things in mind:

1. We must use the exact models we fit to the training set. You fit the logistic regression model in part c and you fit the kNN model in either part c or part g.
2. We should not use the results of the testing predictions to change our models. That should have been done with the training sets.

Find the predictions for the test set using the models, print the output of the `confusionMatrix()` function for each model, and compare the results in a few sentences.


### Part k (5 points)
Even though one method may be better on a given dataset than another, that does not mean that method will always predict better. However, logistic regression has a few advantages over kNN regardless of predictive power. List at least three advantages logistic regression has over kNN.

